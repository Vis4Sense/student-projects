# build_edges_from_openalex.py  (robust)
import json, time, re, sys
from pathlib import Path
import requests
import pandas as pd
from urllib.parse import quote
from tqdm import tqdm

DATA = Path("data/export-data.json")      # æˆ– data/export_data.json
DATA_ALT = Path("data/export_data.json")
OUT_EDGES = Path("data/edges.csv")
OUT_MAP   = Path("data/openalex_map.csv")

# *** å¿…å¡«ï¼šè¯·æ”¹æˆä½ çš„çœŸå®é‚®ç®± ***
MAILTO = "alyqw23.nottingham.ac.uk"
UA     = f"Zotero-RAG-Assistant/0.2 (contact: {MAILTO})"

BASE = "https://api.openalex.org/works"
SESS = requests.Session()
SESS.headers.update({"User-Agent": UA})
TIMEOUT = 25
PAUSE = 0.25   # ç¤¼è²Œé™é€Ÿ

def load_items():
    src = DATA if DATA.exists() else DATA_ALT
    if not src.exists():
        print(f"[ERR] Missing {DATA} / {DATA_ALT}", file=sys.stderr); sys.exit(1)
    raw = json.loads(src.read_text(encoding="utf-8"))
    rows = []
    for it in raw:
        d = it.get("data") if isinstance(it, dict) and "data" in it else it
        if not isinstance(d, dict):
            continue
        pid   = str(d.get("id") or d.get("key") or d.get("itemID") or "").strip()
        title = str(d.get("title") or d.get("shortTitle") or "").strip()
        doi   = str(d.get("DOI")   or d.get("doi") or "").strip()
        year  = str(d.get("year")  or d.get("date") or "")[:4]
        if not pid or not title:
            continue
        doi = re.sub(r"^https?://(dx\.)?doi\.org/", "", doi, flags=re.I) if doi else ""
        rows.append({"id": pid, "title": title, "doi": doi, "year": year})
    return pd.DataFrame(rows)

def get_json(url, params=None):
    p = dict(params or {})
    if MAILTO:
        p.setdefault("mailto", MAILTO)
    r = SESS.get(url, params=p, timeout=TIMEOUT)
    if r.status_code == 404:
        return None
    r.raise_for_status()
    return r.json()

def by_doi_all_ways(doi):
    """è¯• 3 ç§ DOI æŸ¥è¯¢æ–¹å¼ï¼Œä»»ä¸€å‘½ä¸­å³è¿”å› work å¯¹è±¡"""
    # 1) /works/doi:xxxxx
    try:
        j = get_json(f"{BASE}/doi:{quote(doi, safe='')}")
        if j and "id" in j:
            return j
    except requests.HTTPError as e:
        # 400/404 ç»§ç»­å°è¯•
        pass

    # 2) filter=doi:xxxxx ï¼ˆç²¾ç¡®ï¼‰
    try:
        j = get_json(BASE, params={"filter": f"doi:{doi}", "select": "id,doi,display_name,referenced_works"})
        # filter è¿”å› results[]
        if j and isinstance(j.get("results"), list) and j["results"]:
            return j["results"][0]
    except requests.HTTPError:
        pass

    # 3) filter=doi.search:xxxxx ï¼ˆå®½æ¾ï¼‰
    try:
        j = get_json(BASE, params={"filter": f"doi.search:{doi}", "select": "id,doi,display_name,referenced_works"})
        if j and isinstance(j.get("results"), list) and j["results"]:
            return j["results"][0]
    except requests.HTTPError:
        pass

    return None

def by_title_fallback(title, year=""):
    """æ ‡é¢˜æœç´¢å…œåº•ï¼šsearch=titleï¼Œé™„å¸¦å¹´ä»½ç­›ä¸€ç­›"""
    params = {"search": title, "per-page": 3, "select": "id,doi,display_name,publication_year,referenced_works"}
    if year and year.isdigit():
        params["filter"] = f"from_publication_year:{year},to_publication_year:{year}"
    j = get_json(BASE, params=params)
    if j and isinstance(j.get("results"), list) and j["results"]:
        # ç®€å•æ‹¿ç¬¬ä¸€æ¡ï¼›å¯åŠ ç›¸ä¼¼åº¦/å¹´ä»½ç²¾ç¡®åŒ¹é…
        return j["results"][0]
    return None

def main():
    df = load_items()
    if df.empty:
        print("[ERR] No items parsed from export JSON."); return

    # ä»…ç”¨å¸¦ DOI çš„å…ˆåŒ¹é…ï¼›æ—  DOI çš„å›é€€æ ‡é¢˜æœ
    print(f"ğŸ“š Items: {len(df)} (with DOI: {(df['doi']!='').sum()})")
    id_map = {}      # lib_id -> {"doi":..., "openalex":..., "refs":[...]}
    misses = []

    for _, row in tqdm(df.iterrows(), total=len(df)):
        pid, doi, title, year = row["id"], row["doi"], row["title"], row["year"]
        work = None
        if doi:
            work = by_doi_all_ways(doi)
        if work is None:
            # æ ‡é¢˜å…œåº•ï¼ˆå°¤å…¶æ˜¯æ²¡æœ‰ DOI æˆ–ç‰¹æ®Š DOIï¼‰
            try:
                work = by_title_fallback(title, year)
            except Exception as e:
                pass

        if work and work.get("id"):
            refs = work.get("referenced_works") or []
            id_map[pid] = {
                "doi": doi,
                "openalex": work.get("id"),
                "refs": refs
            }
        else:
            misses.append({"id": pid, "title": title, "doi": doi, "year": year})

        time.sleep(PAUSE)

    print(f"âœ… mapped: {len(id_map)} ; âŒ missed: {len(misses)}")
    if not id_map:
        print("æ²¡æœ‰ä»»ä½•æ¡ç›®åŒ¹é…åˆ° OpenAlexã€‚è¯·æ£€æŸ¥ï¼š1) å°† MAILTO æ”¹æˆä½ çš„çœŸå®é‚®ç®±ï¼›2) ç½‘ç»œå¯è®¿é—® openalexï¼›3) æ•°æ®é‡Œæ˜¯å¦æœ‰ DOIã€‚")
        OUT_MAP.parent.mkdir(parents=True, exist_ok=True)
        pd.DataFrame(misses).to_csv(OUT_MAP.with_name("openalex_miss.csv"), index=False)
        return

    # åå‘æ˜ å°„ï¼šä»…åº“å†…
    oa2lib = {v["openalex"]: k for k, v in id_map.items() if v.get("openalex")}
    # ç”Ÿæˆåº“å†…â†’åº“å†…è¾¹
    edges = []
    for lib_id, v in id_map.items():
        src_oa = v.get("openalex")
        if not src_oa:
            continue
        for ref in v.get("refs", []):
            if ref in oa2lib:  # åªä¿ç•™æŒ‡å‘åº“å†…è®ºæ–‡çš„å¼•ç”¨
                edges.append({"citer": lib_id, "cited": oa2lib[ref]})

    OUT_EDGES.parent.mkdir(parents=True, exist_ok=True)
    if edges:
        pd.DataFrame(edges).drop_duplicates().to_csv(OUT_EDGES, index=False)
        print(f"ğŸ§© edges written: {OUT_EDGES}  ({len(edges)} rows, deduped)")
    else:
        # æ²¡æœ‰åº“å†…â†’åº“å†…çš„è¾¹ä¹Ÿç®—æˆåŠŸï¼ˆåªæ˜¯å›¾ä¸ºç©ºï¼‰
        pd.DataFrame(columns=["citer","cited"]).to_csv(OUT_EDGES, index=False)
        print("âš ï¸ æ²¡å½¢æˆåº“å†…â†’åº“å†…çš„å¼•ç”¨ï¼ˆå¯èƒ½éƒ½æŒ‡å‘åº“å¤–ï¼‰ã€‚å·²å†™å…¥ç©º edges.csvã€‚")

    # ä¿å­˜æ˜ å°„è¡¨ä¾¿äºæ’æŸ¥
    rows = [{"id": k, "doi": v.get("doi"), "openalex": v.get("openalex")} for k, v in id_map.items()]
    pd.DataFrame(rows).to_csv(OUT_MAP, index=False)
    if misses:
        pd.DataFrame(misses).to_csv(OUT_MAP.with_name("openalex_miss.csv"), index=False)
        print(f"éƒ¨åˆ†æœªåŒ¹é…ï¼Œè¯¦è§ï¼š{OUT_MAP.with_name('openalex_miss.csv')}")

if __name__ == "__main__":
    main()
